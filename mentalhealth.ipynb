{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gmiserani/mentalhealth/blob/main/mentalhealth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe8I3oYVf26o"
      },
      "outputs": [],
      "source": [
        "USE_TPU = 0\n",
        "USE_GPU = 1\n",
        "\n",
        "assert (USE_TPU == 0) | (USE_GPU == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l38YALyYtlIY",
        "outputId": "f06ed8d4-6c80-4de0-87c9-4fb44fafc9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/MentalHealthShared\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X569UD4ltn9P",
        "outputId": "274184be-eec5-402a-bbb8-76691813ad40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MentalHealthShared\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/MentalHealthShared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZpXXo3jjYL",
        "outputId": "ed6e03b6-0106-48fa-d678-3850df6870df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk>=3.6.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: ratelimit>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (2.2.1)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.28.1)\n",
            "Requirement already satisfied: scikit_learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.8.1+cu111 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (4.64.1)\n",
            "Requirement already satisfied: xgboost>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.6.2)\n",
            "Requirement already satisfied: pyarrow>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (6.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.2->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.2->-r requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.2->-r requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.4->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.4->-r requirements.txt (line 3)) (2022.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (2022.9.24)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.24.2->-r requirements.txt (line 6)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>=0.24.2->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1+cu111->-r requirements.txt (line 7)) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.4->-r requirements.txt (line 3)) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe2utkmuf9zj"
      },
      "outputs": [],
      "source": [
        "ON_COLAB=False\n",
        "if ON_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    BASEDIR='/content/drive/MyDrive/MentalHealthShared'\n",
        "    PYTHONDIR=BASEDIR+'src'\n",
        "    RESULTSDIR=BASEDIR+'results/'\n",
        "    MODELSDIR=BASEDIR+'model/'\n",
        "    DATADIR=BASEDIR+'data/'\n",
        "    import utils \n",
        "    from pre_processing import findLast, convert_utc, check_missing_threads\n",
        "else:\n",
        "    import os\n",
        "    BASEDIR = os.getcwd() + \"/\"\n",
        "    dirs = [\"results\",\"model\",\"data\"]\n",
        "    for dirc in dirs:\n",
        "        if dirc not in os.listdir(): \n",
        "            os.makedirs(os.path.join(BASEDIR,dirc))\n",
        "    PYTHONDIR=BASEDIR+'src/'\n",
        "    RESULTSDIR=BASEDIR+'results/'\n",
        "    MODELSDIR=BASEDIR+'model/'\n",
        "    DATADIR=BASEDIR+'data/'\n",
        "    from src import utils\n",
        "    from src.pre_processing import findLast, convert_utc, check_missing_threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiDD6kQ8gBrq",
        "outputId": "33d2372f-f5a1-4cd1-ac3a-7d1e3efe9a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "if USE_TPU:\n",
        "      !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "      !python pytorch-xla-env-setup.py #--version $VERSION\n",
        "\n",
        "      # imports pytorch\n",
        "      import torch\n",
        "      # imports the torch_xla package\n",
        "      import torch_xla\n",
        "      import torch_xla.core.xla_model as xm\n",
        "\n",
        "      device = xm.xla_device()\n",
        "      torch_xla._XLAC._xla_set_default_device(str(device))\n",
        "\n",
        "else:\n",
        "      import torch\n",
        "      device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTltOAoPgE1K"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(PYTHONDIR)\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import time\n",
        "import pdb\n",
        "import importlib\n",
        "import pprint\n",
        "import re\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuOQjzuHgHwT",
        "outputId": "bad31fc0-e6c2-43d6-fc75-628743f4f96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# loading DistilBERT to obtain embeddings\n",
        "!pip install -q transformers\n",
        "# Use DistilBERT para representar cada post e comentario\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Rqi0ywgKar",
        "outputId": "e83e5408-c8b9-4c81-f2ca-b99ee5ea19f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# loading VADER for sentiment analysis\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# initialize VADER\n",
        "sid = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "def areIntervalsShort(post, seqlen_series):\n",
        "  postid = post.name\n",
        "  print(postid)\n",
        "  if postid in seqlen_series.index:\n",
        "    seqlen = seqlen_series.loc[postid]\n",
        "    print(seqlen)\n",
        "    # from post (created_utc[0]) to last comment (created_utc[seqlen])\n",
        "    timestamps = np.array(post.created_utc[:seqlen])\n",
        "    print(timestamps)\n",
        "    timedeltas = timestamps[1:] - timestamps[:-1]\n",
        "    print(timedeltas)\n",
        "    print(timedelta(days=1))\n",
        "    return np.all(timedeltas < timedelta(days=1))#.total_seconds())\n"
      ],
      "metadata": {
        "id": "R4mrF45Z2vp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inTsnHdXgNY7"
      },
      "outputs": [],
      "source": [
        "def create_subreddit_df(reddit, infile_extension, suffix=''):\n",
        "    infile_extension = infile_extension.lstrip('.')\n",
        "\n",
        "    print('Loading dicts...')\n",
        "    with open(f'{DATADIR}{reddit}_post2data{suffix}.{infile_extension}','rb') as infile:\n",
        "        post2data = pickle.load(infile)\n",
        "    with open(f'{DATADIR}{reddit}_post2comments{suffix}.{infile_extension}','rb') as infile:\n",
        "        post2comments = pickle.load(infile)\n",
        "    with open(f'{DATADIR}{reddit}_comment2data{suffix}.{infile_extension}','rb') as infile:\n",
        "        comment2data = pickle.load(infile)\n",
        "\n",
        "    outpath=f'{DATADIR}{reddit}{suffix}_distilbert_filtered_posts.parquet'\n",
        "    print(f'Dataframe will be saved in {outpath}')\n",
        "\n",
        "    print('creating df from post2data...')\n",
        "    df = pd.DataFrame(list(post2data.values()), index=post2data.keys())\n",
        "    df = df.rename( columns={'selftext':'text'})\n",
        "    print(1, len(df))\n",
        "\n",
        "\n",
        "    #df = df.drop(columns='num_comments')\n",
        "\n",
        "    # join with list of comments\n",
        "    print('joining with post2comments...')\n",
        "    tmp = pd.Series(post2comments,name='comments')\n",
        "    df = df.join(tmp, how='inner')\n",
        "    print(2, len(df))\n",
        "\n",
        "    post_df = df\n",
        "\n",
        "    print('creating df from comment2data...')\n",
        "    df = pd.DataFrame(list(comment2data.values()), index=comment2data.keys())\n",
        "    df = df.rename( columns={'body':'text'})\n",
        "    # THE FOLLOWING 2 LINES MUST BE USED ONLY IF THE LINK_IDS HAVE A PREFIX\n",
        "    \n",
        "    #df[\"link_id\"] = df[\"link_id\"].apply(lambda x:x[3:])\n",
        "    #df[\"parent_id\"] = df[\"parent_id\"].apply(lambda x:x[3:]) \n",
        "    print(3, len(df))\n",
        "\n",
        "\n",
        "    print('remove comments that correspond to inexistent threads...')\n",
        "    #post_ids = set(post_df.index)\n",
        "    #print(post_df.comments)\n",
        "    #print(df.head())\n",
        "    #pdb.set_trace()\n",
        "    lst = post_df.comments.values.tolist()\n",
        "    flat_list = [item for sublist in lst for item in sublist]\n",
        "    post_ids = pd.DataFrame(flat_list, columns = ['col'])\n",
        "    df = df[df.link_id.index.isin(post_ids.col)]\n",
        "    print(4, len(df))\n",
        "\n",
        "    #link dos dois contenham o mesmo id do post e do comentario pra ver se o autor da thread escreveu o comentario\n",
        "    print('compute is_post_author...')\n",
        "    post_df2 = post_df.explode('comments')\n",
        "    #pdb.set_trace()\n",
        "    df = post_df2.join(df, on = 'comments', how = 'right' , rsuffix='_post')\n",
        "    \n",
        "    #df = df.merge(post_df['author'], right_on=['index'], left_on=['comments'] , how=\"inner\", suffixes=('_post'))\n",
        "    df['is_post_author'] = (df.author == df.author_post)\n",
        "    #print(df)\n",
        "    df = df.drop('author_post', axis=1)\n",
        "    post_df = post_df.filter(items=df.index, axis=0)\n",
        "\n",
        "    comment_df = df\n",
        "\n",
        "    print('remove  posts without text...')\n",
        "    post_df = post_df.dropna()\n",
        "    # post_df.num_comments = post_df.num_comments.astype(int)\n",
        "    print(5, len(post_df))\n",
        "\n",
        "    print('remove posts with less than 2 comments...')\n",
        "    # remove posts with less than 2 comments\n",
        "    post_df['num_comments'] = post_df['comments'].apply(len) # use this as num_comments\n",
        "    post_df = post_df[post_df['num_comments'] >= 2]\n",
        "    print(6, len(post_df))\n",
        "\n",
        "    print('remove posts with small text...')\n",
        "    post_df = post_df[post_df['text'].apply(len) >= 2]\n",
        "    print(7, len(post_df))\n",
        "\n",
        "    print('remove removed or deleted posts...')\n",
        "    post_df = post_df[(post_df['text'] != '[removed]') & (post_df['text'] != '[deleted]')]\n",
        "    print(8, len(post_df))\n",
        "\n",
        "\n",
        "    print('remove deleted authors...')\n",
        "    post_df = post_df[(post_df['author'] != '[deleted]')]\n",
        "    print(9, len(post_df))\n",
        "\n",
        "    #pdb.set_trace()\n",
        "    \n",
        "    print('calculate sequence length when there are other commenters followed by post author...')\n",
        "    #pdb.set_trace()\n",
        "    comment_df['post_id'] = comment_df.index\n",
        "    comment_df.set_index('comments', inplace = True)\n",
        "    check_missing_threads(post_df,comment_df)\n",
        "    temp = post_df.apply(lambda x: utils.getThreadLen(x, comment_df),axis=1)\n",
        "    #post_df['seq_len']\n",
        "\n",
        "    print('remove invalid threads...')\n",
        "    post_df = post_df.dropna()\n",
        "    print(10, len(post_df))\n",
        "\n",
        "    print('remove comments without body...')\n",
        "    print(comment_df.loc[comment_df.index == \"dbwc0z7\"])\n",
        "    #comment_df = comment_df.dropna()\n",
        "    print(11,len(comment_df))\n",
        "\n",
        "    print('remove comments that correspond to deleted threads...')\n",
        "    post_ids = set(post_df.index)\n",
        "    comment_df = comment_df[comment_df.post_id.isin(post_ids)]\n",
        "    print(12,len(comment_df))\n",
        "\n",
        "    print('computing score...')\n",
        "    post_df['score']    =    post_df['text'].apply(lambda x: utils.calculateParagraphScore(x,sid))\n",
        "    comment_df['score'] = comment_df['text'].apply(lambda x: utils.calculateParagraphScore(x,sid))\n",
        "\n",
        "    print(\"remove posts whose scores and posts whose comments' scores couldn't be computed...\")\n",
        "    posts_to_remove = list(post_df[post_df.score.isna()].index)\n",
        "    posts_to_remove += list(comment_df[comment_df.score.isna()]['link_id'].values)\n",
        "    post_df = post_df.drop(posts_to_remove)\n",
        "    post_df = post_df.drop_duplicates(subset=\"created_utc\", keep='first')\n",
        "    print(13,len(post_df))\n",
        "\n",
        "    print('remove comments whose posts were removed...')\n",
        "    comment_df = comment_df[~comment_df.link_id.isin(posts_to_remove)]\n",
        "    print(14,len(comment_df))\n",
        "\n",
        "    print('concatenating post and comment columns...')\n",
        "    #print(comment_df.loc[comment_df.index == 'dbwc0z7'])\n",
        "    #post_df['parent_id'] = post_df.apply(lambda x: [comment_df.loc[cid, 'parent_id'] for cid in x.comments], axis=1)\n",
        "    post_df['is_post_author'] = post_df.apply(lambda x: [True] + [comment_df.loc[cid, 'is_post_author'] for cid in x.comments], axis=1)\n",
        "    post_df.score = post_df.apply(lambda x: [x.score] + [comment_df.loc[cid, 'score'] for cid in x.comments], axis=1)\n",
        "    # post_df.score_noavg = post_df.apply(lambda x: [x.score_noavg] + [comment_df.loc[cid, 'score_noavg'] for cid in x.comments], axis=1)\n",
        "    post_df.created_utc = post_df.apply(lambda x: [x.created_utc] + [comment_df.loc[cid, 'created_utc'] for cid in x.comments], axis=1)\n",
        "    print(post_df)\n",
        "\n",
        "    # # ATTENTION! new seq_len includes post and last_comment\n",
        "    # # for instance sequence [p, c0, c1, c2, ..., cn]\n",
        "    # # where cn is the last comment made by author has seq_len=n+2\n",
        "\n",
        "    # compute new seq_len:\n",
        "    print('computing seq_len...')\n",
        "    post_df['seq_len'] = post_df.is_post_author.apply(findLast)+1\n",
        "    post_df['seq_len'] = post_df.seq_len.astype(int)\n",
        "\n",
        "    print('computing DistilBERT features...')\n",
        "    post_df['features'] = utils.extractFeatures(post_df, tokenizer, model, device, batch_size = 512)\n",
        "    comment_df['features'] = utils.extractFeatures(comment_df, tokenizer, model, device, batch_size = 512)\n",
        "\n",
        "    print('concatenating post and comment features...')\n",
        "    post_df.features = post_df.apply(lambda x: [x.features] + [comment_df.loc[cid, 'features'] for cid in x.comments], axis=1)\n",
        "\n",
        "    print('computing additional features for pre-filtering...')\n",
        "    seqlen_series = utils.getNonOverlappingThreads(post_df)\n",
        "    #   post_df = post_df.apply(lambda x: convert_utc(x),axis=1)\n",
        "    #print(post_df)\n",
        "    #are_intervals_short = post_df.apply(lambda x: areIntervalsShort(x,seqlen_series), axis=1)\n",
        "    #are_intervals_short = are_intervals_short.dropna()\n",
        "    \n",
        "\n",
        "    #seqlen_series = seqlen_series[(seqlen_series>=2) & are_intervals_short]\n",
        "    seqlen_series = seqlen_series[(seqlen_series>=2)]\n",
        "    seqlen_series.name = 'filtered_seqlen'\n",
        "    post_df = post_df.join(seqlen_series)\n",
        "\n",
        "    validbranches_series = post_df.apply(utils.getValidBranches,axis=1)\n",
        "    validbranches_series.name = 'valid_branches'\n",
        "    post_df = post_df.join(validbranches_series)\n",
        "\n",
        "    columns_to_drop = [column for column in ['pos','neg','neu','compound','num_comments','score_noavg'] if column in post_df.columns]\n",
        "    if columns_to_drop:\n",
        "        print('Dropping superfluous columns...')\n",
        "        post_df = post_df.drop(columns=columns_to_drop)\n",
        "\n",
        "    pdb.set_trace()\n",
        "    post_df.to_parquet(outpath)\n",
        "\n",
        "    return post_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wrmfDy6ygSUD",
        "outputId": "95e7ff62-bbcf-42cf-b1b4-79f98b80edc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dicts...\n",
            "Dataframe will be saved in /content/drive/MyDrive/MentalHealthShared/data/SuicideWatch_distilbert_filtered_posts.parquet\n",
            "creating df from post2data...\n",
            "1 250\n",
            "joining with post2comments...\n",
            "2 233\n",
            "creating df from comment2data...\n",
            "3 1475\n",
            "remove comments that correspond to inexistent threads...\n",
            "4 1475\n",
            "compute is_post_author...\n",
            "remove  posts without text...\n",
            "5 1475\n",
            "remove posts with less than 2 comments...\n",
            "6 1432\n",
            "remove posts with small text...\n",
            "7 1401\n",
            "remove removed or deleted posts...\n",
            "8 854\n",
            "remove deleted authors...\n",
            "9 770\n",
            "calculate sequence length when there are other commenters followed by post author...\n",
            "Checking for missing threads in comment_df...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [00:00, 596.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r120it [00:00, 557.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbuzq9e is missing from comment_df... Deleting thread\n",
            "Comment:dbv0dp3 is missing from comment_df... Deleting thread\n",
            "Comment:dbv1zr9 is missing from comment_df... Deleting thread\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r176it [00:00, 526.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n",
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n",
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r229it [00:00, 492.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "451it [00:00, 506.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n",
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n",
            "Comment:dbuzq9e is missing from comment_df... Deleting thread\n",
            "Comment:dbv0dp3 is missing from comment_df... Deleting thread\n",
            "Comment:dbv1zr9 is missing from comment_df... Deleting thread\n",
            "Comment:dbv0gei is missing from comment_df... Deleting thread\n",
            "Comment:dbv0p23 is missing from comment_df... Deleting thread\n",
            "Comment:dbv192h is missing from comment_df... Deleting thread\n",
            "Comment:dbv1hxo is missing from comment_df... Deleting thread\n",
            "Comment:dbv23cf is missing from comment_df... Deleting thread\n",
            "Comment:dbv2741 is missing from comment_df... Deleting thread\n",
            "Comment:dbv6ioo is missing from comment_df... Deleting thread\n",
            "Comment:dbvgjdu is missing from comment_df... Deleting thread\n",
            "Comment:dbvut35 is missing from comment_df... Deleting thread\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "652it [00:01, 616.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "770it [00:01, 548.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "Comment:dbxq8d5 is missing from comment_df... Deleting thread\n",
            "remove invalid threads...\n",
            "10 751\n",
            "remove comments without body...\n",
            "             author  created_utc  num_comments  \\\n",
            "comments                                         \n",
            "dbwc0z7   Sbliguess   1483343719             7   \n",
            "\n",
            "                                                       text  \\\n",
            "comments                                                      \n",
            "dbwc0z7   I've Always prefered being alone most of the t...   \n",
            "\n",
            "                                                  text_post  created_utc_post  \\\n",
            "comments                                                                        \n",
            "dbwc0z7   Yeah, I also realized one day that I was contr...        1483345228   \n",
            "\n",
            "            link_id  parent_id  is_post_author post_id  \n",
            "comments                                                \n",
            "dbwc0z7   338543491        NaN           False  5lk5tv  \n",
            "11 1475\n",
            "remove comments that correspond to deleted threads...\n",
            "12 751\n",
            "computing score...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove posts whose scores and posts whose comments' scores couldn't be computed...\n",
            "13 102\n",
            "remove comments whose posts were removed...\n",
            "14 751\n",
            "concatenating post and comment columns...\n",
            "                      author  \\\n",
            "5lk5tv             Sbliguess   \n",
            "5ljqgf            Scutigera_   \n",
            "5ldbdm      throw_away_posts   \n",
            "5lhnvr        letsplayyatzee   \n",
            "5ldpz3          SleepyEnigma   \n",
            "...                      ...   \n",
            "5lmxx3       autisticshit124   \n",
            "5lnbh0          Chrisboy1990   \n",
            "5ln97q  PsyCosocIaL_Stranger   \n",
            "5lo4jn         glittershines   \n",
            "5lohqb       throwawaywwwwwe   \n",
            "\n",
            "                                              created_utc  num_comments  \\\n",
            "5lk5tv  [1483343719, 1483343719, 1483343719, 148334371...             7   \n",
            "5ljqgf  [1483336883, 1483336883, 1483336883, 148333688...             5   \n",
            "5ldbdm  [1483241256, 1483241256, 1483241256, 148324125...             8   \n",
            "5lhnvr  [1483310407, 1483310407, 1483310407, 148331040...            30   \n",
            "5ldpz3  [1483248019, 1483248019, 1483248019, 148324801...            12   \n",
            "...                                                   ...           ...   \n",
            "5lmxx3  [1483384129, 1483384129, 1483384129, 148338412...             6   \n",
            "5lnbh0  [1483388117, 1483388117, 1483388117, 148338811...             4   \n",
            "5ln97q  [1483387453, 1483387453, 1483387453, 148338745...             7   \n",
            "5lo4jn               [1483396713, 1483396713, 1483396713]             2   \n",
            "5lohqb   [1483400809, 1483400809, 1483400809, 1483400809]             3   \n",
            "\n",
            "                                                     text  \\\n",
            "5lk5tv  I've Always prefered being alone most of the t...   \n",
            "5ljqgf  I'm a 15 year old boy. I've had diagnosed depr...   \n",
            "5ldbdm  I don't have the courage, but the urge is grow...   \n",
            "5lhnvr  I'm a 33yr old male from Northwest Indiana tha...   \n",
            "5ldpz3  Family obligations, school, arguments in the f...   \n",
            "...                                                   ...   \n",
            "5lmxx3  You probably saw me in this sub a few days ago...   \n",
            "5lnbh0  My girlfriend broke up with me the other day, ...   \n",
            "5ln97q  So, on saturday night, I was at home alone and...   \n",
            "5lo4jn  Things have been really difficult for me latel...   \n",
            "5lohqb  So, redditors of SuicideWatch,\\n\\nMight as wel...   \n",
            "\n",
            "                                                 comments  \\\n",
            "5lk5tv  [dbwc0z7, dbwc505, dbwca27, dbwceil, dbwmgi4, ...   \n",
            "5ljqgf      [dbw8xlw, dbw9bni, dbwjzot, dbwpvyd, dbwr7go]   \n",
            "5ldbdm  [dbuuakj, dbuumqi, dbuutd7, dbuuyfp, dbuv76z, ...   \n",
            "5lhnvr  [dbvre16, dbvvnac, dbvvyo8, dbvw1l1, dbvwd32, ...   \n",
            "5ldpz3  [dbuxgqh, dbuxjvt, dbuxtph, dbuxzhb, dbuz3u3, ...   \n",
            "...                                                   ...   \n",
            "5lmxx3  [dbx2vmp, dby95ly, dbz9dco, dc22578, dcby7e5, ...   \n",
            "5lnbh0               [dbwydq1, dbwyg3y, dbx8tgq, dbxu2pt]   \n",
            "5ln97q  [dbwzfoi, dbwzqmm, dbx00tf, dbx3s6z, dbx405m, ...   \n",
            "5lo4jn                                 [dbx51uz, dbxem4a]   \n",
            "5lohqb                        [dbx8hmw, dbxdj1s, dbxk1g2]   \n",
            "\n",
            "                                                    score  \\\n",
            "5lk5tv  [0.13506, 0.13506, 0.13506, 0.13506, 0.13506, ...   \n",
            "5ljqgf  [-0.024509090909090925, -0.024509090909090925,...   \n",
            "5ldbdm  [0.3896, 0.3896, 0.3896, 0.3896, 0.3896, 0.389...   \n",
            "5lhnvr  [0.02865333333333334, 0.02865333333333334, 0.0...   \n",
            "5ldpz3  [-0.37042, -0.37042, -0.37042, -0.37042, -0.37...   \n",
            "...                                                   ...   \n",
            "5lmxx3  [-0.28874999999999995, -0.28874999999999995, -...   \n",
            "5lnbh0  [0.11941249999999999, 0.11941249999999999, 0.1...   \n",
            "5ln97q  [-0.08704, -0.08704, -0.08704, -0.08704, -0.08...   \n",
            "5lo4jn  [-0.2786571428571429, -0.2786571428571429, -0....   \n",
            "5lohqb  [-0.014209999999999999, -0.014209999999999999,...   \n",
            "\n",
            "                                           is_post_author  \n",
            "5lk5tv  [True, False, True, False, True, False, True, ...  \n",
            "5ljqgf            [True, False, True, False, True, False]  \n",
            "5ldbdm  [True, False, True, False, True, False, True, ...  \n",
            "5lhnvr  [True, False, True, False, False, True, True, ...  \n",
            "5ldpz3  [True, False, True, False, False, False, True,...  \n",
            "...                                                   ...  \n",
            "5lmxx3   [True, False, False, False, False, False, False]  \n",
            "5lnbh0                  [True, False, True, False, False]  \n",
            "5ln97q  [True, False, True, False, True, False, True, ...  \n",
            "5lo4jn                               [True, False, False]  \n",
            "5lohqb                        [True, False, False, False]  \n",
            "\n",
            "[102 rows x 7 columns]\n",
            "computing seq_len...\n",
            "computing DistilBERT features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "100%|██████████| 2/2 [00:05<00:00,  2.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "concatenating post and comment features...\n",
            "computing additional features for pre-filtering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97/97 [00:00<00:00, 522.37it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-d63641d53d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSUBREDDITS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"SuicideWatch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubreddit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSUBREDDITS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcreate_subreddit_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-f83d9cd66c85>\u001b[0m in \u001b[0;36mcreate_subreddit_df\u001b[0;34m(reddit, infile_extension, suffix)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mpost_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqlen_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mvalidbranches_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValidBranches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mvalidbranches_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid_branches'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mpost_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidbranches_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/MentalHealthShared/src/utils.py\u001b[0m in \u001b[0;36mgetValidBranches\u001b[0;34m(post)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetValidBranches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m   \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'is_post_author'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_post_author\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'comment_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'parent_id'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parent_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m   \u001b[0;31m# for name, group in grouped:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;31m#   print(group)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'parent_id'"
          ]
        }
      ],
      "source": [
        "\n",
        "import pdb\n",
        "from tqdm import tqdm\n",
        "SUBREDDITS = [\"SuicideWatch\"]\n",
        "for subreddit in SUBREDDITS:\n",
        "    create_subreddit_df(subreddit,'pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RQx0ijlw1Ow"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMO1LqJgVRc"
      },
      "source": [
        "Experiments on the effect of ettiquete words\n",
        "THE NEXT SECTION IS DEVOTED TO THE EXPERIMENTS DONE IN REGARDS TO THE EFFECT THAT ETTIQUETE WORDS HAVE ON VADER. NOT NECESSARY FOR THE MAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bztVVC1LgZKN"
      },
      "outputs": [],
      "source": [
        "RUN_ETTIQUETE_EXPS = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtEV63zEgciV"
      },
      "outputs": [],
      "source": [
        "def filter_etiquette(subreddits,return_filtered_subreddits=False,etiquette_list = [\"Thank You\",\"Thank you\",\"Thanks\",\"Thx\",\"Thankyou\", \"thank you\", \"thanks\", \"thx\", \"thankyou\"],file_type = '_comment2data',file_extension='p'):\n",
        "    if(type(subreddits) != type(list())):\n",
        "        subreddits = [subreddits]\n",
        "    print(\"Initializing...\")\n",
        "    total_perc = []\n",
        "    filtered_subs = {}\n",
        "    for subreddit in subreddits:\n",
        "        print(f\"Getting r/{subreddit} data...\")\n",
        "        with open(f'{DATADIR}no_thx_2017/{subreddit}_{file_type}.{file_extension}','rb') as infile:\n",
        "            curr_sub_comments = pickle.load(infile)\n",
        "        num_comments_with_etiquette = 0\n",
        "        total_comments = len(curr_sub_comments)\n",
        "        for key,comment in tqdm(curr_sub_comments.items()):\n",
        "            for etiquette_word in etiquette_list:\n",
        "                if etiquette_word in comment['text']:\n",
        "                    num_comments_with_etiquette += 1\n",
        "                    curr_sub_comments[str(key)]['text'] = comment['text'].replace(etiquette_word,\"\")\n",
        "            if re.search(r\"\\b\" + re.escape(\"ty\") + r\"\\b\",comment['text']):\n",
        "                num_comments_with_etiquette += 1\n",
        "                curr_sub_comments[str(key)]['text'] = re.sub(r\"\\b\" + re.escape(\"ty\") + r\"\\b\",\"\",comment['text'])\n",
        "            if re.search(r\"\\b\" + re.escape(\"Ty\") + r\"\\b\",comment['text']):\n",
        "                num_comments_with_etiquette += 1\n",
        "                curr_sub_comments[str(key)]['text'] = re.sub(r\"\\b\" + re.escape(\"Ty\") + r\"\\b\",\"\",comment['text'])\n",
        "        \n",
        "        print(f\"Percentage of posts or comments filtered in r/{subreddit}: {(num_comments_with_etiquette/total_comments) * 100}%\")\n",
        "        total_perc.append([num_comments_with_etiquette,total_comments])\n",
        "        print(\"Saving changes...\")\n",
        "        #pickle.dump(curr_sub_comments,open(DATADIR+ \"no_thx_2017/\" + f'{subreddit}_{file_type}.{file_extension}','wb'))\n",
        "        filtered_subs[subreddit] = curr_sub_comments\n",
        "        print(f\"Done with r/{subreddit}\")\n",
        "    print(f\"Total Percentage of posts or comments with etiquette words: {sum([total_perc[x][0] for x in range(len(total_perc))])/sum([total_perc[x][1] for x in range(len(total_perc))]) * 100}%\")\n",
        "    print(\"Done!\")\n",
        "    if(return_filtered_subreddits):\n",
        "        return filtered_subs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmWmnXOMgffu"
      },
      "outputs": [],
      "source": [
        "if RUN_ETTIQUETE_EXPS:\n",
        "    subreddits = ['Anxiety','bipolar','depression','SuicideWatch']\n",
        "    filter_etiquette(subreddits,file_type=\"comment2data\",return_filtered_subreddits=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1do9ORNhItm85Bda-pGqvOyXbwe4NFG9l",
      "authorship_tag": "ABX9TyM9R45l33LIzW0rJjn81G7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}